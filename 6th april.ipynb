{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ba212b-f200-4b89-a437-34a78502255e",
   "metadata": {},
   "source": [
    "# Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcc4a6-5f4e-4065-a408-5f0215e3cd21",
   "metadata": {},
   "source": [
    "## A linear SVM (Support Vector Machine) is a type of SVM that uses a linear hyperplane to separate two classes of data. The mathematical formula for a linear SVM can be expressed as follows:\n",
    "## Given a training set of input vectors X and corresponding class labels y, where X = {x1, x2, ..., xn}, xi ∈ R^d and y ∈ {-1, +1}, the linear SVM seeks to find a hyperplane in the d-dimensional input space that separates the two classes.\n",
    "## The hyperplane is defined by the equation: w^T x + b = 0\n",
    "## where w is the weight vector perpendicular to the hyperplane, and b is the bias term.\n",
    "## The distance between the hyperplane and the closest point in each class is called the margin. The objective of the linear SVM is to maximize the margin while minimizing the classification error.\n",
    "## This can be formulated as the optimization problem: minimize ||w||^2/2 subject to y_i(w^T x_i + b) >= 1 for all i\n",
    "## where ||w|| is the Euclidean norm of the weight vector w.\n",
    "## This is a convex optimization problem, and can be solved using quadratic programming techniques.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20edc35d-8804-4837-a52c-52bab9697d71",
   "metadata": {},
   "source": [
    "# Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1c509-0692-4855-8e36-31411d0d1830",
   "metadata": {},
   "source": [
    "## The objective function of a linear SVM (Support Vector Machine) is to find the hyperplane that maximizes the margin between two classes of data while minimizing the classification error.\n",
    "## The margin is defined as the distance between the hyperplane and the closest point in each class. The hyperplane that maximizes the margin is chosen because it provides the best generalization performance on unseen data.\n",
    "## The objective function of a linear SVM can be formulated as a constrained optimization problem: minimize ||w||^2/2\n",
    "## subject to y_i(w^T x_i + b) >= 1 for all i\n",
    "## where ||w|| is the Euclidean norm of the weight vector w, and b is the bias term. The constraint ensures that the hyperplane correctly classifies all training examples, with a margin of at least 1.\n",
    "## The objective function seeks to minimize the norm of the weight vector while satisfying the classification constraint. By minimizing the norm of the weight vector, the objective function encourages the SVM to find a sparse solution, where only a small number of input features are used to separate the classes. This helps to avoid overfitting and improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0d3eb-f22c-4231-8c68-ebdd303f76a8",
   "metadata": {},
   "source": [
    "# Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13aa70-543b-48b0-b457-5bf78305b852",
   "metadata": {},
   "source": [
    "## The kernel trick is a technique used in Support Vector Machines (SVMs) to transform the input data into a higher-dimensional space in order to find a better separating hyperplane. Instead of computing the dot product between input vectors directly, the kernel function computes the dot product between the vectors in a higher-dimensional feature space.\n",
    "## The kernel function maps the input data into a higher-dimensional space where it may be more separable. This is done without actually computing the transformation to the higher-dimensional space explicitly, which can be computationally expensive or even impossible if the dimensionality is very high.\n",
    "## The kernel trick allows the SVM to learn non-linear decision boundaries in the original input space, even though the optimization problem is formulated in terms of a linear hyperplane. This is achieved by replacing the dot product of input vectors with a kernel function that implicitly computes the dot product in a higher-dimensional space.\n",
    "## The most commonly used kernel functions in SVMs are the linear kernel, polynomial kernel, and radial basis function (RBF) kernel. The choice of kernel function depends on the specific problem and the properties of the data.\n",
    "## In summary, the kernel trick is a powerful technique that allows SVMs to handle complex data by transforming it into a higher-dimensional feature space without explicitly computing the transformation. This enables the SVM to find a better separating hyperplane and improve classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e761d5-4fd7-4a5b-8194-2bd177ae83d8",
   "metadata": {},
   "source": [
    "# Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00937a88-5b55-4126-9aca-cceecaf78749",
   "metadata": {},
   "source": [
    "## Support vectors are the data points that lie closest to the separating hyperplane in Support Vector Machines (SVMs). These are the data points that have the largest margin, and they play a crucial role in the SVM's decision boundary.\n",
    "## In SVM, the decision boundary is determined by the weights assigned to each feature in the input data. The support vectors are the training examples that are closest to the decision boundary and they determine the orientation of the hyperplane.\n",
    "## During the training process, the SVM algorithm seeks to find the hyperplane that maximizes the margin between the two classes while correctly classifying all training examples. The margin is defined as the distance between the hyperplane and the closest point in each class. The training examples that lie on the margin or violate the classification constraint are not used in determining the orientation of the hyperplane.\n",
    "## Once the optimal hyperplane is found, the support vectors are the data points that are closest to the hyperplane and provide the most information about the problem. These support vectors determine the position and orientation of the decision boundary and are used to make predictions on new data.\n",
    "## For example, consider a binary classification problem where the input data is two-dimensional and consists of two classes that are not linearly separable. The SVM algorithm transforms the data into a higher-dimensional feature space using a kernel function, and finds the hyperplane that maximizes the margin between the two classes.\n",
    "## The support vectors are the data points that lie closest to the decision boundary and determine the orientation of the hyperplane. These support vectors are crucial in predicting the class of new data points that lie close to the decision boundary.\n",
    "## In summary, support vectors play a crucial role in SVMs by determining the position and orientation of the decision boundary. They are the training examples that are closest to the decision boundary and provide the most information about the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4882ca-5e42-45fb-974e-007b93305392",
   "metadata": {},
   "source": [
    "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d4a79-befe-40c0-9fff-4f18534e317f",
   "metadata": {},
   "source": [
    "## 1. Hyperplane: In SVM, the hyperplane is a decision boundary that separates the input data into two classes. The hyperplane is defined as the plane that maximizes the margin between the two classes. The hyperplane can be linear or non-linear, depending on the kernel function used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f9520-afd0-473f-926f-fbe8e52095bc",
   "metadata": {},
   "source": [
    "## 2. Marginal plane: The marginal plane is the plane parallel to the hyperplane that passes through the support vectors. The marginal plane is used to define the margin between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a38dc7-27a1-4c4a-aba4-24e4ef889a6c",
   "metadata": {},
   "source": [
    "## 3. Hard margin: In SVM, the hard margin refers to the case where the SVM algorithm requires that all training examples be correctly classified with a margin of at least 1. In other words, there is no tolerance for misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3dcf7-96fb-47d7-bf7e-056db89ea875",
   "metadata": {},
   "source": [
    "## 4. Soft margin: In SVM, the soft margin refers to the case where the SVM algorithm allows for some misclassification in order to find a better separating hyperplane. The goal is to find a hyperplane that maximizes the margin between the two classes while minimizing the number of misclassified examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7728ac-7c35-451a-b308-961da4c1c1b5",
   "metadata": {},
   "source": [
    "# Q6. SVM Implementation through Iris dataset.\n",
    "# Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "# performance with the scikit-learn implementation.\n",
    "# ~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing set\n",
    "# ~ Train a linear SVM classifier on the training set and predict the labels for the testing set\n",
    "# ~ Compute the accuracy of the model on the testing set\n",
    "# ~ Plot the decision boundaries of the trained model using two of the features\n",
    "# ~ Try different values of the regularisation parameter C and see how it affects the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ae099b-8b0e-4b48-a7dd-fbddff877e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e88182b-886a-4d23-b739-f4cddf292a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0377d3c4-6024-4878-9408-fdbc5c569547",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c0d715-a622-4451-8107-ebeee86d448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b62e2ac9-939a-4317-bb01-dbb418f563e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e1253b-5949-40a2-b562-028392c34d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3490d1c-14d2-43a7-ab3f-8af1e8e7a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Independent & dependent features \n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde68002-32e6-420b-83a5-8807e7c4501e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "250596d0-1b8a-4d3c-b10b-c1e3ffed6c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea90de6a-f224-4f97-90f4-1ad655845d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e2e337-cb6d-48d3-bcd4-8229f7502746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 4), (50, 4), (100,), (50,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e08d2e5b-e33e-415d-8973-0618d551a1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the SVM model\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "922552c2-ccb8-4c10-ae53-31d9e45cc20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#prediction \n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "924ae678-d4db-4c3b-8bff-2a72cf225947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1, Mean accuracy = 0.98, Standard deviation = 0.02\n",
      "C = 0.5, Mean accuracy = 0.98, Standard deviation = 0.02\n",
      "C = 1, Mean accuracy = 0.98, Standard deviation = 0.02\n",
      "C = 5, Mean accuracy = 0.98, Standard deviation = 0.02\n",
      "C = 10, Mean accuracy = 0.98, Standard deviation = 0.02\n",
      "C = 50, Mean accuracy = 0.98, Standard deviation = 0.02\n",
      "C = 100, Mean accuracy = 0.98, Standard deviation = 0.02\n"
     ]
    }
   ],
   "source": [
    "# regularization parameters\n",
    "from sklearn.model_selection import cross_val_score\n",
    "C_values = [0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "\n",
    "# Loop over different C values and compute cross-validation scores\n",
    "for C in C_values:\n",
    "    scores = cross_val_score(model, data.data, data.target, cv=5)\n",
    "    print(\"C = {}, Mean accuracy = {:.2f}, Standard deviation = {:.2f}\".format(\n",
    "        C, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0a5b6-3ba6-485d-8661-81cfd541b23d",
   "metadata": {},
   "source": [
    "# Different C values do not effect much on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f05c9b55-c2b4-4b61-8479-e770abb2630d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 23) (1497170885.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 23\u001b[0;36m\u001b[0m\n\u001b[0;31m    In this code, we first load the iris dataset using scikit-learn's load_iris function. We then choose the first two features of the dataset (X = iris.data[:, :2]) and the target variable (y = iris.target) for plotting.\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 23)\n"
     ]
    }
   ],
   "source": [
    "# plotting of decision boundaries\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# Train an SVM model\n",
    "svm = SVC(kernel='linear', C=1)\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Define a grid of points to plot the decision boundaries\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundaries and the data points\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision boundaries of SVM on iris dataset')\n",
    "plt.show()\n",
    "In this code, we first load the iris dataset using scikit-learn's load_iris function. We then choose the first two features of the dataset (X = iris.data[:, :2]) and the target variable (y = iris.target) for plotting.\n",
    "\n",
    "We then train an SVM model with a linear kernel and C=1 using scikit-learn's SVC class, and fit it to the data using the fit method.\n",
    "\n",
    "Next, we define a grid of points (xx and yy) using the Meshgrid function, and use the predict method of the trained SVM to assign a class label to each point on the grid. We reshape the predicted labels to the same shape as the grid (Z = Z.reshape(xx.shape)).\n",
    "\n",
    "Finally, we use a contour plot (plt.contourf(xx, yy, Z, alpha=0.4)) to visualize the decision boundaries of the SVM model, and scatter plot (plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)) to plot the data points. We also add labels to the x-axis and y-axis, a title to the plot, and display the plot using plt.show().\n",
    "\n",
    "The output of this code might look like this:\n",
    "\n",
    "svm_decision_boundaries_iris.png\n",
    "\n",
    "As we can see from the plot, the decision boundaries of the SVM model are linear, since we used a linear kernel. The plot shows how the SVM model separates the different classes of iris based on the values of the chosen features (feature 1 and feature 2). We can see that the model is able to separate the blue class (setosa) from the other two classes (versicolor and virginica) quite well, but there is some overlap between the green and\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7241f1-f571-4816-8080-3532655d1367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d3f7d7-92db-414d-b0da-6baf82f6a5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
